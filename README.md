# Learning_NLP

ğŸ“˜ Natural Language Processing (NLP) â€“ Learning Notes

Welcome to my Natural Language Processing (NLP) learning repository!
This space contains my notes, explanations, experiments, and summaries of important NLP concepts that I am studying.

ğŸ§  What is NLP?

Natural Language Processing (NLP) is a branch of Artificial Intelligence (AI) that enables computers to understand, interpret, and generate human language.
It combines:

Linguistics
Machine Learning
Deep Learning

ğŸ“š Topics I Am Learning

âœ”ï¸ 1. Text Preprocessing
  Tokenization
  Stopword Removal
  Stemming vs Lemmatization
  Lowercasing
  Removing punctuation
  Normalization

âœ”ï¸ 2. Text Representation
  Bag of Words (BoW)
  One-hot encoding
  TF-IDF
  Word Embeddings
  Word2Vec
  CBOW
  Skip-Gram
  GloVe
  Sentence Embeddings

âœ”ï¸ 3. Word2Vec Concepts
  Context window
  CBOW (predict target using context)
  Skip-Gram (predict context using target)
  Negative Sampling
  Normalized vectors
  Cosine similarity

âœ”ï¸ 4. Dimensionality Reduction
  PCA (Principal Component Analysis)
  t-SNE (visualization)

âœ”ï¸ 5. Text Classification Basics
  Naive Bayes
  Logistic Regression
  SVM
  Training / testing splits
  Evaluation metrics

âœ”ï¸ 6. Advanced NLP (Future Learning)
  RNNs / LSTMs / GRUs
  Transformers
  Attention mechanism
  BERT, GPT
  Machine Translation


ğŸ§ª Hands-on Practice
  I am practicing the learned concepts using small experiments like:
  Tokenizing a dataset
  Building BoW and TF-IDF vectors
  Training a Word2Vec embedding model
  Visualizing word embeddings with PCA
  Simple sentiment analysis models


ğŸ› ï¸ Tools & Libraries Used
  Python
  NLTK
  SpaCy
  Gensim
  Scikit-learn
  NumPy / Pandas
  Matplotlib / Seaborn

ğŸ¯ Goals of This Learning Journey
  Build a strong NLP foundation
  Understand how text data is transformed into numbers
  Learn classic and modern NLP approaches
  Prepare for advanced deep-learning NLP models

  
ğŸŒŸ Future Plans
  Move to deep learning NLP
  Explore HuggingFace Transformers
  Try fine-tuning a BERT model
  Build a real NLP project later
